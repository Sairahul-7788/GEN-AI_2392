{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52043/GENERATIVE-AI-2025/blob/main/2303A52043_ASSIGNMENT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1"
      ],
      "metadata": {
        "id": "qv42FFxmY8Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "YActual YP red\n",
        "20 20.5\n",
        "30 30.3\n",
        "40 40.2\n",
        "50 50.6\n",
        "60 60.7\n",
        "Tabela 1: YActual Vs. YP red'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "U2r1SKiKWN-1",
        "outputId": "cff93253-cd00-4b5b-cc83-680c3f543b1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\\nvalues and deep learning model predicted values are shown in Table 1. Also compare the results\\nwith the outcomes of libraries\\nYActual YP red\\n20 20.5\\n30 30.3\\n40 40.2\\n50 50.6\\n60 60.7\\nTabela 1: YActual Vs. YP red'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuA5RuHH6Nna",
        "outputId": "323708cc-5732-4c37-ea27-8b9ad4099256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom (Scratch) Error Metrics:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-squared (R²): 0.99877\n",
            "\n",
            "Sklearn Error Metrics:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "R-squared (R²): 0.99877\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "Y_actual = np.array([20, 30, 40, 50, 60])\n",
        "Y_pred = np.array([20.5, 30.3, 40.2, 50.6, 60.7])\n",
        "\n",
        "def mean_absolute_error_scratch(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def mean_squared_error_scratch(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def root_mean_squared_error_scratch(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error_scratch(y_true, y_pred))\n",
        "\n",
        "def r2_score_scratch(y_true, y_pred):\n",
        "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
        "    return 1 - (ss_residual / ss_total)\n",
        "\n",
        "mae_scratch = mean_absolute_error_scratch(Y_actual, Y_pred)\n",
        "mse_scratch = mean_squared_error_scratch(Y_actual, Y_pred)\n",
        "rmse_scratch = root_mean_squared_error_scratch(Y_actual, Y_pred)\n",
        "r2_scratch = r2_score_scratch(Y_actual, Y_pred)\n",
        "\n",
        "mae_sklearn = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_sklearn = mean_squared_error(Y_actual, Y_pred)\n",
        "rmse_sklearn = np.sqrt(mse_sklearn)\n",
        "r2_sklearn = r2_score(Y_actual, Y_pred)\n",
        "\n",
        "print(f\"Custom (Scratch) Error Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_scratch}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_scratch}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_scratch}\")\n",
        "print(f\"R-squared (R²): {r2_scratch}\")\n",
        "\n",
        "print(\"\\nSklearn Error Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_sklearn}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_sklearn}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_sklearn}\")\n",
        "print(f\"R-squared (R²): {r2_sklearn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "RlmWHD2rZCfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''(1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red'''"
      ],
      "metadata": {
        "id": "pbjK8FWaXUZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d03724da-d92b-4cf4-93bf-8572b2c62e10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\\nActual values and deep learning model predicted values are shown in Table 2. Also compare the\\nresults with outcome of libraries\\nYActual YP red\\n0 0 1 1 2 0\\n0 0 1 0 2 0\\n0 1 1 2 2 1\\n0 2 1 0 2 2\\n0 2 1 2 2 2\\nTabela 2: YActual Vs. YP red'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix as cm_sklearn\n",
        "def calculate_metrics(y_actual, y_pred, num_classes):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for true, pred in zip(y_actual, y_pred):\n",
        "        cm[true, pred] += 1\n",
        "    acc = accuracy_score(y_actual, y_pred)\n",
        "    print(f\"Accuracy: {acc}\")\n",
        "    for class_label in range(num_classes):\n",
        "        tp, fp, fn = cm[class_label, class_label], cm[:, class_label].sum() - cm[class_label, class_label], cm[class_label, :].sum() - cm[class_label, class_label]\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "        print(f\"Class {class_label}: Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "y_actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
        "y_pred = [0, 0, 1, 2, 2, 1, 0, 2, 0, 2, 0, 0, 1, 2, 2]\n",
        "num_classes = 3\n",
        "calculate_metrics(y_actual, y_pred, num_classes)\n",
        "print(\"\\nUsing scikit-learn:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_actual, y_pred)}\")\n",
        "print(f\"Precision (per class): {precision_score(y_actual, y_pred, average=None, labels=[0, 1, 2])}\")\n",
        "print(f\"Recall (per class): {recall_score(y_actual, y_pred, average=None, labels=[0, 1, 2])}\")\n",
        "print(f\"F1 Score (per class): {f1_score(y_actual, y_pred, average=None, labels=[0, 1, 2])}\")\n",
        "print(f\"Confusion Matrix:\\n{cm_sklearn(y_actual, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "G2PHjC1EXUj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241aff24-7e40-412f-9f26-a45b04e2f68a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333333333333333\n",
            "Class 0: Precision: 0.3333333333333333, Recall: 0.4, F1 Score: 0.3636363636363636\n",
            "Class 1: Precision: 0.3333333333333333, Recall: 0.2, F1 Score: 0.25\n",
            "Class 2: Precision: 0.3333333333333333, Recall: 0.4, F1 Score: 0.3636363636363636\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 1 2]\n",
            " [2 1 2]\n",
            " [2 1 2]]\n",
            "\n",
            "Using scikit-learn:\n",
            "Accuracy: 0.3333333333333333\n",
            "Precision (per class): [0.33333333 0.33333333 0.33333333]\n",
            "Recall (per class): [0.4 0.2 0.4]\n",
            "F1 Score (per class): [0.36363636 0.25       0.36363636]\n",
            "Confusion Matrix:\n",
            "[[2 1 2]\n",
            " [2 1 2]\n",
            " [2 1 2]]\n"
          ]
        }
      ]
    }
  ]
}